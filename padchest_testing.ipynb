{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "from google.cloud import storage\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import matplotlib.pyplot as plt\n",
    "import faiss\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "print(type(openai.api_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"srobb-datathon\"\n",
    "local_base_path = \"./padchest_sample\" \n",
    "gcs_base_path = \"padchest\"\n",
    "metadata_gcs_path = f\"gs://{bucket_name}/{gcs_base_path}/metadata/chest_x_ray_images_labels_sample.csv\"\n",
    "\n",
    "# init client\n",
    "client = storage.Client()\n",
    "bucket = client.bucket(bucket_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load CSV directly from GCS\n",
    "df = pd.read_csv(metadata_gcs_path)\n",
    "df[\"Report\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['ImageID', 'Labels', 'Report']].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a sample row\n",
    "sample = df.iloc[0]\n",
    "image_id = sample[\"ImageID\"]\n",
    "report = sample[\"Report\"]\n",
    "labels = sample.get(\"Labels\", \"Unknown\")  # fallback in case column is named differently\n",
    "\n",
    "# pull image from GCS\n",
    "image_path = f\"padchest/images/{image_id}\"\n",
    "blob = bucket.blob(image_path)\n",
    "image_data = blob.download_as_bytes()\n",
    "\n",
    "# show image + report\n",
    "img = Image.open(io.BytesIO(image_data))\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.title(f\"Labels: {labels}\")\n",
    "plt.show()\n",
    "\n",
    "print(\"Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab a report from your dataset\n",
    "report_text = df[\"Report\"].iloc[1]\n",
    "print(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab two reports from your dataset\n",
    "report_texts = [df[\"Report\"].iloc[0], df[\"Report\"].iloc[1]]\n",
    "print(report_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You're a clinical assistant reviewing radiology reports.\n",
    "\n",
    "Below are two radiology reports. For each one, extract key clinical findings.\n",
    "\n",
    "Return ONLY a valid Python dictionary, using this format:\n",
    "{{1: [...], 2: [...]}}\n",
    "\n",
    "Do not include any explanation, markdown, or extra text—only the dictionary.\n",
    "\n",
    "Reports:\n",
    "1.\n",
    "\\\"\\\"\\\"{report_texts[0]}\\\"\\\"\\\"\n",
    "\n",
    "2.\n",
    "\\\"\\\"\\\"{report_texts[1]}\\\"\\\"\\\"\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "clinical_findings = eval(response.choices[0].message.content)\n",
    "print(clinical_findings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Is an Image Memory?\n",
    "\n",
    "An `ImageMemory` is a structured, semantically meaningful representation of a medical imaging event. It acts like a *memory cell* for an agent, containing everything needed to understand, compare, and reason about a specific case.\n",
    "\n",
    "Think of it as the AI’s version of a clinical chart entry—but embedded with intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Components of an Image Memory\n",
    "\n",
    "| Field         | Purpose                                                                 |\n",
    "|---------------|-------------------------------------------------------------------------|\n",
    "| `image_id`    | Unique identifier for the image                                         |\n",
    "| `gcs_path`    | Cloud location to access the raw image                                  |\n",
    "| `report`      | Free-text radiology report associated with the image                   |\n",
    "| `label`       | Diagnostic label(s), structured (e.g., “pneumonia”, “cardiomegaly”)     |\n",
    "| `findings`    | Extracted short phrases summarizing key observations                   |\n",
    "| `embedding`   | 1536-d vector capturing semantic meaning of findings/report             |\n",
    "\n",
    "---\n",
    "\n",
    "### Conceptually:\n",
    "An `ImageMemory` is to a radiologist’s brain what a chunked memory is to an agent:\n",
    "> \"I’ve seen something like this before—it had similar findings, here’s what it looked like, and here’s what I thought at the time.\"\n",
    "\n",
    "It bridges:\n",
    "- Visual data (X-ray)\n",
    "- Text data (report, findings)\n",
    "- Structured logic (labels, embeddings)\n",
    "- Agentic cognition (memory, reflection, recall)\n",
    "\n",
    "---\n",
    "\n",
    "### Why This Is Powerful:\n",
    "It gives your agent the ability to think longitudinally, not just answer in the moment.\n",
    "\n",
    "Imagine future steps:\n",
    "- Retrieve similar past ImageMemories based on semantic similarity\n",
    "- Reflect: “What makes this case different from those?”\n",
    "- Flag: “Findings are rare—are we missing something?”\n",
    "- Summarize: “Out of 40 similar cases, 30 were diagnosed with X”\n",
    "\n",
    "---\n",
    "\n",
    "Would you like to refine the data structure further or keep it lean and flexible for now?\n",
    "\n",
    "Once we're aligned on that, we can decide how we want to store and query these ImageMemories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab reports and images for first two rows\n",
    "samples = df.iloc[:2]\n",
    "\n",
    "# build memory objects\n",
    "image_memories = []\n",
    "\n",
    "for i, (index, row) in enumerate(samples.iterrows(), start=1):\n",
    "    memory = {\n",
    "        \"image_id\": row[\"ImageID\"],\n",
    "        \"gcs_path\": f\"gs://srobb-datathon/padchest/images/{row['ImageID']}\",\n",
    "        \"report\": row[\"Report\"],\n",
    "        \"label\": row.get(\"Labels\", \"unknown\"),\n",
    "        \"findings\": clinical_findings.get(i, [])\n",
    "    }\n",
    "    image_memories.append(memory)\n",
    "\n",
    "# display result\n",
    "image_memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize FAISS\n",
    "dimension = 1536\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "memory_map = {}\n",
    "\n",
    "# loop over all memory objects\n",
    "for idx, memory in enumerate(image_memories):\n",
    "    # build embedding input\n",
    "    findings_text = \"; \".join(memory[\"findings\"])\n",
    "    \n",
    "    # get embedding\n",
    "    embedding_response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=findings_text\n",
    "    )\n",
    "    embedding = embedding_response.data[0].embedding\n",
    "    memory[\"embedding\"] = embedding\n",
    "\n",
    "    # add to FAISS\n",
    "    vec = np.array([embedding], dtype=\"float32\")\n",
    "    index.add(vec)\n",
    "\n",
    "    # map index position to memory\n",
    "    memory_map[idx] = memory\n",
    "\n",
    "print(f\"Embedded and indexed {len(image_memories)} ImageMemories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick one memory to simulate as the query (can be new or existing)\n",
    "query_memory = image_memories[1]  # you could try 0, 1, or a new one later\n",
    "query_vec = np.array([query_memory[\"embedding\"]], dtype=\"float32\")\n",
    "\n",
    "# search for the top 1 most similar stored memory\n",
    "D, I = index.search(query_vec, k=1)\n",
    "\n",
    "# unpack results\n",
    "matched_index = I[0][0]\n",
    "matched_distance = D[0][0]\n",
    "matched_memory = memory_map[matched_index]\n",
    "\n",
    "# display\n",
    "print(\"Search our memory for a match:\")\n",
    "print(f\"- Image ID: {matched_memory['image_id']}\")\n",
    "print(f\"- Distance Score: {matched_distance:.4f}\")\n",
    "print(f\"- Findings: {matched_memory['findings']}\")\n",
    "print(f\"- Report: {matched_memory['report']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search top 2 to include the self-match\n",
    "D, I = index.search(query_vec, k=2)\n",
    "\n",
    "print(\"Search results:\")\n",
    "for rank in range(2):\n",
    "    idx = I[0][rank]\n",
    "    dist = D[0][rank]\n",
    "    match = memory_map[idx]\n",
    "    print(f\"\\nMatch {rank + 1}:\")\n",
    "    print(f\"- Image ID: {match['image_id']}\")\n",
    "    print(f\"- Distance Score: {dist:.4f}\")\n",
    "    print(f\"- Findings: {match['findings']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import open_clip\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# --- Load the model & tokenizer ---\n",
    "# model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:mgbam/OpenCLIP-BiomedCLIP-Finetuned')\n",
    "# tokenizer = open_clip.get_tokenizer('hf-hub:mgbam/OpenCLIP-BiomedCLIP-Finetuned')\n",
    "# model.eval()\n",
    "\n",
    "model, preprocess_train, preprocess_val = open_clip.create_model_and_transforms('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "tokenizer = open_clip.get_tokenizer('hf-hub:microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224')\n",
    "model.eval()\n",
    "\n",
    "# --- Load and preprocess your image ---\n",
    "image_path = \"./data/padchest_sample/images/\" + image_memories[0][\"image_id\"]\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_input = preprocess_val(image).unsqueeze(0)\n",
    "\n",
    "# --- Define text prompts ---\n",
    "prompts = [\"normal chest x-ray\", \"aortic elongation\", \"signs of COPD\"]\n",
    "text_inputs = tokenizer(prompts)\n",
    "\n",
    "# --- Encode both ---\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    text_features = model.encode_text(text_inputs)\n",
    "\n",
    "# --- Normalize (important for cosine similarity) ---\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "# --- Compute similarity ---\n",
    "similarity = (image_features @ text_features.T).squeeze(0)\n",
    "\n",
    "# --- Show results ---\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"{prompt}: {similarity[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess second image (Memory 1)\n",
    "image_path = \"./data/padchest_sample/images/\" + image_memories[1][\"image_id\"]\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "image_input = preprocess_val(image).unsqueeze(0)\n",
    "\n",
    "# Re-run embedding and similarity code below\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image_input)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    similarity = (image_features @ text_features.T).squeeze(0)\n",
    "\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(f\"{prompt}: {similarity[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ImageMemory = {\n",
    "    \"image_id\": \"...png\",\n",
    "    \"gcs_path\": \"...\",\n",
    "    \"report\": \"...\",\n",
    "    \"label\": \"...\",\n",
    "    \"findings\": [...],\n",
    "    \"text_embedding\": [...],     # OpenAI embedding of report or findings\n",
    "    \"image_embedding\": [...],    # BiomedCLIP embedding of image\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the same model/tokenizer you already have loaded\n",
    "# openai_client already set up as `client`\n",
    "\n",
    "for mem in image_memories:\n",
    "    # --- TEXT EMBEDDING ---\n",
    "    findings_text = \"; \".join(mem[\"findings\"])\n",
    "    text_response = client.embeddings.create(\n",
    "        model=\"text-embedding-3-small\",\n",
    "        input=findings_text\n",
    "    )\n",
    "    mem[\"text_embedding\"] = text_response.data[0].embedding\n",
    "\n",
    "    # --- IMAGE EMBEDDING ---\n",
    "    image_path = f\"./data/padchest_sample/images/{mem['image_id']}\"\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = preprocess_val(image).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        img_emb = model.encode_image(image_tensor)\n",
    "        img_emb /= img_emb.norm(dim=-1, keepdim=True)  # normalize\n",
    "        mem[\"image_embedding\"] = img_emb.squeeze().cpu().tolist()  # convert to list for storage\n",
    "\n",
    "print(\"All ImageMemories now contain both text and image embeddings.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mem in enumerate(image_memories):\n",
    "    has_text = \"text_embedding\" in mem and isinstance(mem[\"text_embedding\"], list)\n",
    "    has_image = \"image_embedding\" in mem and isinstance(mem[\"image_embedding\"], list)\n",
    "    print(f\"ImageMemory {i}: Text → {has_text} | Image → {has_image}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TEXT INDEX SETUP\n",
    "dim = len(image_memories[0][\"text_embedding\"])\n",
    "text_index = faiss.IndexFlatL2(dim)\n",
    "text_id_map = {}\n",
    "\n",
    "# Populate index\n",
    "for i, mem in enumerate(image_memories):\n",
    "    vec = np.array([mem[\"text_embedding\"]], dtype=\"float32\")\n",
    "    text_index.add(vec)\n",
    "    text_id_map[i] = mem\n",
    "\n",
    "print(f\"Stored {text_index.ntotal} text embeddings in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Memory 1's text_embedding as a query\n",
    "query_vec = np.array([image_memories[1][\"text_embedding\"]], dtype=\"float32\")\n",
    "D, I = text_index.search(query_vec, k=1)\n",
    "\n",
    "# Retrieve match\n",
    "matched_index = I[0][0]\n",
    "matched_distance = D[0][0]\n",
    "matched_memory = text_id_map[matched_index]\n",
    "\n",
    "print(\"Closest Match by Report:\")\n",
    "print(f\"- Image ID: {matched_memory['image_id']}\")\n",
    "print(f\"- Distance: {matched_distance:.4f}\")\n",
    "print(f\"- Findings: {matched_memory['findings']}\")\n",
    "print(f\"- Report: {matched_memory['report']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search top 2 to include the self-match\n",
    "D, I = text_index.search(query_vec, k=2)\n",
    "\n",
    "print(\"Search results:\")\n",
    "for rank in range(2):\n",
    "    idx = I[0][rank]\n",
    "    dist = D[0][rank]\n",
    "    match = memory_map[idx]\n",
    "    print(f\"\\nMatch {rank + 1}:\")\n",
    "    print(f\"- Image ID: {match['image_id']}\")\n",
    "    print(f\"- Distance Score: {dist:.4f}\")\n",
    "    print(f\"- Findings: {match['findings']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMAGE INDEX SETUP\n",
    "dim = len(image_memories[0][\"image_embedding\"])\n",
    "image_index = faiss.IndexFlatL2(dim)\n",
    "image_id_map = {}\n",
    "\n",
    "print(\"Search results:\")\n",
    "for rank in range(2):\n",
    "    idx = I[0][rank]\n",
    "    dist = D[0][rank]\n",
    "    match = memory_map[idx]\n",
    "    print(f\"\\nMatch {rank + 1}:\")\n",
    "    print(f\"- Image ID: {match['image_id']}\")\n",
    "    print(f\"- Distance Score: {dist:.4f}\")\n",
    "    print(f\"- Findings: {match['findings']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mem in image_memories:\n",
    "    text_vec = np.array(mem[\"text_embedding\"], dtype=\"float32\")\n",
    "    image_vec = np.array(mem[\"image_embedding\"], dtype=\"float32\")\n",
    "    \n",
    "    # concatenate\n",
    "    multimodal_vec = np.concatenate([text_vec, image_vec])\n",
    "    mem[\"multimodal_embedding\"] = multimodal_vec.tolist()\n",
    "\n",
    "print(\"Added multimodal_embedding to all ImageMemories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTIMODAL INDEX\n",
    "dim = len(image_memories[0][\"multimodal_embedding\"])\n",
    "multimodal_index = faiss.IndexFlatL2(dim)\n",
    "multimodal_id_map = {}\n",
    "\n",
    "# Add to FAISS\n",
    "for i, mem in enumerate(image_memories):\n",
    "    vec = np.array([mem[\"multimodal_embedding\"]], dtype=\"float32\")\n",
    "    multimodal_index.add(vec)\n",
    "    multimodal_id_map[i] = mem\n",
    "\n",
    "print(f\"Stored {multimodal_index.ntotal} multimodal embeddings in FAISS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare query vector from Memory 1\n",
    "query_vec = np.array([image_memories[1][\"multimodal_embedding\"]], dtype=\"float32\")\n",
    "\n",
    "# search top 2 to include self-match\n",
    "D, I = multimodal_index.search(query_vec, k=2)\n",
    "\n",
    "# display results\n",
    "print(\"Multimodal Search Results:\")\n",
    "for rank in range(2):\n",
    "    idx = I[0][rank]\n",
    "    dist = D[0][rank]\n",
    "    match = multimodal_id_map[idx]\n",
    "    print(f\"\\nMatch {rank + 1}:\")\n",
    "    print(f\"- Image ID: {match['image_id']}\")\n",
    "    print(f\"- Distance Score: {dist:.4f}\")\n",
    "    print(f\"- Findings: {match['findings']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the two cases: self and top match\n",
    "query = image_memories[1]\n",
    "match = image_memories[0]  # top non-self match\n",
    "\n",
    "# Create prompt for agentic reflection\n",
    "prompt = f\"\"\"\n",
    "You are a clinical assistant comparing two radiology cases.\n",
    "\n",
    "Case A (new input):\n",
    "- Findings: {query['findings']}\n",
    "- Report: {query['report']}\n",
    "\n",
    "Case B (closest past memory):\n",
    "- Findings: {match['findings']}\n",
    "- Report: {match['report']}\n",
    "\n",
    "Please reflect on the similarity and difference between these two cases.\n",
    "Summarize in plain language what they have in common, and what is clinically distinct.\n",
    "\"\"\"\n",
    "\n",
    "# Call GPT-4o\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    temperature=0.2,\n",
    ")\n",
    "\n",
    "print(\"Agent Reflection:\\n\")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
